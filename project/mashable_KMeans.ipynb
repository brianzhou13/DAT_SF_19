{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#still need to split the data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# allow plots to appear in the noteboo\n",
    "\n",
    "# create a custom colormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure,show,output_notebook\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.io import hplot, output_file\n",
    "from sklearn.naive_bayes import MultinomialNB #naive-bayes\n",
    "\n",
    "from sklearn.cross_validation import ShuffleSplit #seems like this is needed for random forests\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#ROC Curve + Confusion_Matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA #SVD\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "mash = pd.read_csv(\"OnlineNewsPopularity.csv\") #remember that read_csv auto places read data into a DF\n",
    "\n",
    "#noticed that the headers all had a whitespace in it, so needed to remove it.\n",
    "mash.columns = mash.columns.str.strip()\n",
    "\n",
    "mash['url_id'] = mash.index \n",
    "\n",
    "#let's move the url_id column from the end to the top.\n",
    "\n",
    "cols = mash.columns.tolist()\n",
    "cols\n",
    "\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "cols\n",
    "\n",
    "mash = mash[cols]\n",
    "\n",
    "exc_list_labels = ['timedelta','kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', \n",
    "                   'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
    "                  'self_reference_avg_sharess', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'url_id']\n",
    "\n",
    "mash.drop('n_non_stop_words', axis = 1)\n",
    "\n",
    "for value in exc_list_labels:\n",
    "    mash = mash.drop([value], axis = 1)\n",
    "\n",
    "\n",
    "mash.info()\n",
    "\n",
    "what_day_append = []\n",
    "\n",
    "#We are going to do 'categorize each day with a value\n",
    "\n",
    "\n",
    "#each article has multiple values\n",
    "\n",
    "for value in range(len(mash)):\n",
    "    \n",
    "    if(mash.weekday_is_monday[value] == 1):\n",
    "        what_day_append.append(1) #if monday, then 1\n",
    "    \n",
    "    elif(mash.weekday_is_tuesday[value] == 1 ):\n",
    "        what_day_append.append(2) #if tuesday, then 2\n",
    "        \n",
    "    elif(mash.weekday_is_wednesday[value] == 1):\n",
    "        what_day_append.append(3) #if wednesday, then 3\n",
    "        \n",
    "    elif(mash.weekday_is_thursday[value] == 1):\n",
    "        what_day_append.append(4) #if thursday, then 4\n",
    "        \n",
    "    elif(mash.weekday_is_friday[value] == 1):\n",
    "        what_day_append.append(5) #if friday, then 5\n",
    "        \n",
    "    elif(mash.weekday_is_saturday[value] == 1):\n",
    "        what_day_append.append(6) #if saturday, then 6\n",
    "        \n",
    "    elif(mash.weekday_is_sunday[value] == 1):\n",
    "        what_day_append.append(7) #if sunday, then 7\n",
    "        \n",
    "    elif(mash.is_weekend[value] == 1):\n",
    "        what_day_append.append(8) #if it's a weekend, then 8\n",
    "        \n",
    "    else:\n",
    "        what_day_append.append(0) #shouldn't be any 0 values\n",
    "        \n",
    "len(what_day_append)\n",
    "\n",
    "#mash.info()\n",
    "\n",
    "mash['day'] = what_day_append\n",
    "mash.day.value_counts() #see that there are no 0's\n",
    "\n",
    "mash_life = mash[mash.data_channel_is_lifestyle == 1]\n",
    "mash_ent = mash[mash.data_channel_is_entertainment == 1]\n",
    "mash_bus = mash[mash.data_channel_is_bus == 1]\n",
    "mash_socmed = mash[mash.data_channel_is_socmed == 1]\n",
    "mash_tech = mash[mash.data_channel_is_tech == 1]\n",
    "mash_world = mash[mash.data_channel_is_world == 1]\n",
    "\n",
    "x = len(mash_life)\n",
    "x2 = len(mash_ent)\n",
    "x3 = len(mash_bus)\n",
    "x4 = len(mash_socmed)\n",
    "x5 = len(mash_tech)\n",
    "x6 = len(mash_world)\n",
    "\n",
    "#print x + x2 + x3 + x4 + x5 + x6 #33510\n",
    "#print len(mash) #39644\n",
    "\n",
    "#we are missing ~ 6000 data points that are unlabeled. \n",
    "\n",
    "rng_of_na = []\n",
    "\n",
    "for value in range(len(mash)):\n",
    "    if (mash.data_channel_is_lifestyle[value] != 1):\n",
    "        if(mash.data_channel_is_entertainment[value] != 1):\n",
    "            if(mash.data_channel_is_bus[value] != 1):\n",
    "                if(mash.data_channel_is_socmed[value] != 1):\n",
    "                    if(mash.data_channel_is_tech[value] != 1):\n",
    "                        if(mash.data_channel_is_world[value] != 1):\n",
    "                            rng_of_na.append(value) #we will have all the index\n",
    "                            \n",
    "\n",
    "na = len(rng_of_na)\n",
    "#print na\n",
    "#print x + x2 + x3 + x4 + x5 + x6 + na #all blog posts are now accounted for. \n",
    "\n",
    "m_n_1 = mash[mash.data_channel_is_lifestyle != 1]\n",
    "m_n_2 = m_n_1[m_n_1.data_channel_is_entertainment != 1]\n",
    "m_n_3 = m_n_2[m_n_2.data_channel_is_bus != 1]\n",
    "m_n_4 = m_n_3[m_n_3.data_channel_is_socmed !=1]\n",
    "m_n_5 = m_n_4[m_n_4.data_channel_is_tech != 1]\n",
    "mash_na = m_n_5[m_n_5.data_channel_is_world !=1]\n",
    "\n",
    "len(mash_na)\n",
    "\n",
    "#remove the data channel and weekday labels\n",
    "\n",
    "columns_remove_channels = ['data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', \n",
    "                  'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world']\n",
    "\n",
    "columns_remove_days = ['weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', \n",
    "                 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend']\n",
    "\n",
    "for value, value2 in zip(columns_remove_channels, columns_remove_days):\n",
    "    mash_life = mash_life.drop([value], axis = 1)\n",
    "    mash_life = mash_life.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_ent = mash_ent.drop([value], axis = 1)\n",
    "    mash_ent = mash_ent.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_bus = mash_bus.drop([value], axis = 1)\n",
    "    mash_bus = mash_bus.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_socmed = mash_socmed.drop([value], axis = 1)\n",
    "    mash_socmed = mash_socmed.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_tech = mash_tech.drop([value], axis = 1)\n",
    "    mash_tech = mash_tech.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_world = mash_world.drop([value], axis = 1)\n",
    "    mash_world = mash_world.drop([value2], axis = 1)\n",
    "    \n",
    "    mash_na= mash_na.drop([value], axis = 1)\n",
    "    mash_na = mash_na.drop([value2], axis = 1)\n",
    "    \n",
    "#mash = mash.drop(columns_remove_channels, axis = 1)\n",
    "#mash = mash.drop(columns_remove_days, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mash_life' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c7cbf94085af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmash_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmash_life\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmash_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmash_bus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmash_socmed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmash_tech\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmash_world\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mname_of_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mash_life'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'mash_ent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mash_bus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mash_socmed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mash_tech'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mash_world'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mash_life' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score #this is the valuation score when we use KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mash_lists = [mash_life, mash_ent, mash_bus, mash_socmed, mash_tech, mash_world]  \n",
    "name_of_lists = ['mash_life' , 'mash_ent', 'mash_bus', 'mash_socmed', 'mash_tech', 'mash_world'] \n",
    "\n",
    "\n",
    "km_list_of_cvs = [] #this will take in all the values of the cross_val_score for each dataset\n",
    "km_list_of_cm = []\n",
    "km_list_of_cr = []\n",
    "\n",
    "km_list_of_centers = [] #this will take in all the values of the centroids\n",
    "km_list_of_labels = [] #this will take in all the values of the labels\n",
    "km_list_of_s_scores = []\n",
    "\n",
    "std_scalar = StandardScaler() #standardization\n",
    "\n",
    "count = 0 #used for the naming\n",
    "\n",
    "## beginning of for-loop\n",
    "\n",
    "for mash_testing in mash_lists:\n",
    "    \n",
    "    print \"Below is analysis on %s dataset\" % name_of_lists[count]\n",
    "    \n",
    "    features = mash_testing.drop(['shares', 'url'], axis=1)\n",
    "    target = mash_testing['shares']\n",
    "    \n",
    "    target_sorted = sorted(target) #sorting it so we can get the top 75%\n",
    "    \n",
    "    target_len = len(target_sorted)\n",
    "    target_len_75 = (target_len/4)*3\n",
    "    \n",
    "    val_at_75 = target_sorted[target_len_75] #found the value at index of 75%\n",
    "    \n",
    "    for value in features.index: \n",
    "        if(target[value] >= val_at_75):\n",
    "            target[value] = 1\n",
    "        else:\n",
    "            target[value] = 0\n",
    "    \n",
    "    #end for-loop\n",
    "    features_sc = std_scalar.fit_transform(features)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_sc, target, test_size = 0.3, random_state=1)\n",
    "    \n",
    "    #end splitting into training data\n",
    "    km_test_list = []\n",
    "    km_test_labels = []\n",
    "    km_test_scores = []\n",
    "    \n",
    "    for val in range(2, 6):\n",
    "        \n",
    "        km = KMeans(val) #applied PCA earlier to come up with 2\n",
    "        km.fit(X_train, y_train)\n",
    "    \n",
    "        km_centers = km.cluster_centers_ #these are our centroids\n",
    "        #km_list_of_centers.append(km_centers)\n",
    "        km_test_list.append(km_centers)\n",
    "        #print \"km_centers for %s\" % name_of_lists[count]\n",
    "        #print km_centers\n",
    "    \n",
    "        km_labels = km.labels_\n",
    "        km_test_labels.append(km_labels)\n",
    "        #km_list_of_labels.append(km_labels)\n",
    "        #print \"km_labels for %s\" % name_of_lists[count]\n",
    "        #print km_labels\n",
    "    \n",
    "        s_score = silhouette_score(X_train,km_labels,metric='euclidean')\n",
    "        km_test_scores.append(s_score)\n",
    "        #km_list_of_s_scores.append(s_score)\n",
    "    \n",
    "        print \"s_score for %s\" % val\n",
    "        print s_score\n",
    "    \n",
    "    print \"Plot for %s\" % name_of_lists[count]\n",
    "    \n",
    "    x_axis = range(2,6)\n",
    "    \n",
    "    plt.plot(x_axis, km_test_scores)  #x should be a range from 2 to 6\n",
    "    #plt.ylabel('s_score')\n",
    "    plt.show()\n",
    "    \n",
    "    km_list_of_centers.append(km_test_list)\n",
    "    km_list_of_labels.append(km_test_labels)\n",
    "    km_list_of_s_scores.append(km_test_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### other reporting / excluding due to exploration\n",
    "    \n",
    "    cvs = cross_val_score(km, X_train, y_train).mean()\n",
    "    km_list_of_cvs.append(cvs)\n",
    "    print \"Cross Val Scores for %s\" % name_of_lists[count]\n",
    "    print cvs\n",
    "#    \n",
    "    km_pred = km.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, km_pred)\n",
    "    km_list_of_cm.append(cm)\n",
    "    print \"Confusion Matrix Below for %s\" % name_of_lists[count]\n",
    "    print cm\n",
    "#    \n",
    "    cr = classification_report(y_test, km_pred)\n",
    "    km_list_of_cr.append(cr)\n",
    "    print \"Classification Report Below for %s\" % name_of_lists[count]\n",
    "    print classification_report(y_test, km_pred)\n",
    "    \n",
    "    count = count + 1\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
